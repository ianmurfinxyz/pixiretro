- need to take into account different color modes when drawing sprites - at min always take
the color from the sprite resource independent of the color mode.

how to handle the font loading?

  - bitmap font generator can generate the image with the RGB channels all white (255) for the entire
    image and the alpha channel being 255 for glyph pixels and 0 for non-glyph pixels (empty space).
    I can then load this format into a sprite instance (seperating each glyph into their own sprite
    or I can store the sheet in the font and have the draw text function just use the sheet; the
    latter way will prob be better as it will mean fewer vector instances and therefore fewer
    small allocations, less fragmentation and less allocation time)
  - since I use an alpha value of 0 as the key for non-drawn pixels those empty space pixels
    when copied to a screen will not be drawn event though the RGB channels will be set to 255 
    on the screen (or I could just not copy the pixels to the screen but this would require checks
    on every pixel)
  - in color mode rgb the text would be drawn white since that is the color of each pixel in the
    bitmap, in color mode bitmaps, all pixels with alpha channel values != alpha key (i.e. 0)
    will adopt the bitmap color. This will allow me to draw fonts in any color.
  - this method will also allow me to use the banded color modes when drawing text. (side note
    that in si the ui doesnt use color banding, thus the ui layer will want to be in rgb mode 
    while the stage layer will want to be in banding mode).
  - can use tiny xml to load font data and data for each glyph then use bmpimage to load the
    glyph image (will need to convert glyph image to a bmp 32bit RGBA with gimp as bitmap font
    generator prog only exports png). The xml file contains the offset, sizes and advances etc
    I need to space all glyphs.
  - Bitmap font generator is doing a really bad job of generating a png for a small 8px font, 
    thus I think I will need to make the image myself and set the data in the xml file myself
    for the font. This will be the base little font used in the game. May be able to use 
    the font generator for larger fonts that it wont butcher, thus should copy the xml format
    so I can load both custom hand written xml font files and the ones generated by the font
    generator program.


    NEXT STEP THEN:
      HAND MAKE MY FONT IMAGE AND THE XML
      IMPLEMENT THE FONT LOADER AND FONT RENDERER IN GFX
      UPDATE THE BITMAP CLASS TO STORE THE WIDTH HEIGHT DATA IN A SIZE VECTOR2I TO BRING INLINE 
      WITH CONVENTIONS USED ELSEWHERE IN THE ENGINE CODE BASE.

  - had another idea to support animations - should change ResourceKey_t for sprites to being
    for sprite sheets and then in the function to drawSprite should include a frame parameter
    like:
        drawSprite(Vector2i position, TilesetID setid, int frame, Layer layer)
                                          ^
                                          |
                                          tileset is a better name than sprite sheet.

                                              or how about spriteset

                                              SpritesetID    <-- yh I like this even better
                                                                 since they are not really tiles
                                                                 but frames in a set, the set
                                                                 can be any set though.

    and create an xml file for the sheet which like for fonts contains meta data for each frame
    in the sheet as well as for the sheet itself, so the filename for the image, the number of
    frames in the sheet, and for the frames, the positions in the image and the frame number.

    This will make loading sprites analagous to loading fonts in that both assets use an xml file
    for meta data and an associated bmp file for the pixel data.

    This would make it easier to implement large animation sets such as needed if I were to
    implement a game like galaga which uses frames for different rotation states.

    would prob still hardcode the expected number of frames in each sheet and what each frame
    corresponds to so would have to match up the data in the xml file and the bmp file with the
    hardcoded expectations in the game code.

  - can also get rid of the sprite class and just have a tileset class which contains the pixel
    data stored in a bmpimage instance (bmpimage and sprite are basically exactly the same in 
    terms of member data), and then store the metadata for the frames within the tileset class.

  
DONE: 26th jan
  - implemented sprite sheets and their loader

TODO
  - refactor draw sprite to use the sprite sheets instead (will need to pass a frame)
  - compile, debug and test new bmpimage and gfx code
  - implement font loading and rendering
  - compile and test

TODO
  - thoroughly test the asset loaders by trying to break them - will want to verify font data
    by checking all glyph coordinates are within the bmp file to avoid later seg faults. Keep
    varying the errors in the xml file to see how my loader handles them. Want to handle them
    gracefully and avoid crashes.

  - handle the different color modes when redering and implement further rendering functions.

TODO - 29th jan
  - IT RUNS LIKE SHIT!! NEED TO IMPROVE THE PERFORMANCE OF THE RENDERING. THIS WILL NOT DO!
  - After timing some of the code it is the glClear call which is taking around 15-20ms to return.
    This thread explains why this happens:
      https://stackoverflow.com/questions/29551516/glclear-takes-too-long-android-opengl-es-2
    Basically my draw calls are too expensive. The CPU is able to issue the calls far faster than
    the GPU is able to render them, thus the glClear call is running some sync method to stall
    the CPU until the GPU can catch up. Solution is clearly to reduce the demand on the GPU.

    Good news is that my text rendering function was not the problem thus that doesn't need 
    optimising.

    note: this likely also explains why the profiling tools I was using (gprof and valgrind) were 
    not very helpful and did not reveal this problem. They were telling me it was the text rendering 
    function that was taking all the time. This is likely that it was taking all the CPU time (the 
    most expensive function run on the CPU) and the profilers were not measuring the time the CPU 
    was waiting for the GPU.

  - Ok so I reduce the demand on the GPU and it made no difference! So my next best guess would
    be that the GPU is intentionally limiting draw rate to sync with the screen. 
    pen -> yep it appears to be due to vsync being enabled on my intel integrated graphics GPU.

    If I run the engine on my old laptop with intel i3 M370 integrated graphics it runs at a 
    limit of 60Hz. Running on my current computer with intel i5 ? with HD Graphics 4600 it runs
    at a limit of 50Hz. Although note that it is the screen refresh rate which the FPS is being
    clamped to.

    This explains why timing each gl call shows the net result of the timings is always ~20ms
    (to result in 50Hz) or ~16ms (to result in 60Hz) on my old laptop. If I reduce the demand on
    the GPU the glClear call stalls, if I increase the demand on GPU via other gl calls the
    glClear call returns much faster. If I overload the other calls the glClear call returns
    in ~10us thus it is not the glClear call but the driver artifically limitiing frame rate.

    Running the 'glxgears' program from the 'mesa-demos' (arch) package shows this is the case.
    The demo renders some gears at the capped vsync rate. The rate capped to in the demo matches
    the rate my engine is capped to on both this machine and my old laptop.

    This post explains how to lift the vsync restriction:
      https://www.microbasic.net/2015/04/how-to-disable-vsync-on-intel-card-linux/

    It is interesting to note however that the vsync seems to be doing a rather poor job as the
    rendering is very flickery on this machine at 50Hz whereas on my old laptop at 60Hz it is
    not. Thus I can only conlude this machines vsync is actually out of sync.

    Yup running the engine example with:
      $ vblank_mode=0 ./example

    runs it without vsync and it runs at capped tick rate with FPS of 980Hz. Without the 
    vsyn the current example runs at a cap of ~750Hz. So performance is not that bad after all.

    +- I was wrong on the vsync being out of sync. Movement is still very flickery even with 
    |  unlimited frame rate. However on my old laptop it is not. The difference is that on my old
    |  laptop the moving object clearly jumps 1 virtual pixel at a time (which on the scale of the
WRONG  demo is ~6 real pixels) thus rather than smoothly moving across the screen it looks like it
    |  is hopping across at regular intervals. On this machine (the newer one) the movement is smooth
    |  with no hopping. You cannot tell that it is actually moving 6px at a time, but it looks very
    |  flickery as a result. My guess is that there is some sort of GPU feature that is trying to
    +- smooth the movement. Either that or it is the screen.

          the flickeryness is actually just a product of the movement speed it would appear. The
          object on my laptop was running slower because in my engine the update rate was 1000Hz
          but the vsync was clamping it to 60Hz, thus if I integrate over a dt of 1/1000 secs but
          do so only 60 time a second ... things move slower and thus flicker less -> dipshit ey!

     +-    BOOM! just discovered that if I lock the engine to draw at the same rate of the screen then
     |     the Graphics driver does not enable vsync, if I try to draw at a higher rate than the screen
     |     refresh rate it does. Thus if I lock my engine to only draw the game at 60Hz and I am running
 WRONG     the game on a screen that draws at 60Hz then simply running ./example without the vblank_mode=0
     |     still results in no vsync. If I then lock the engine draws to 70Hz and run ./example then
     |     the GPU will start blocking in the gl calls again to lock the draw rate the the 60Hz monitor
     |     refresh rate. This is why I was encountering this problem. Because I was drawing at 60Hz but 
     +-    the monitor I was running on refeshed at 50Hz thus the GPU driver was enabling vsync. -> NOPE!
           wrong it does start blocking still. So ignore that theory :). 

            I will leave it here to remind me its not the case.

 Conclusion of all this, just disable vsync during development and carry on.


 - impelement the animation modes
 - setup drawing sprites from the center position rather than the bottom-left - pass the center into
  the gfx draw function? Make a seperate function? gfx::drawSpriteCentered(Vector2i position, ...)
 - update the loader to load the new sprite xml format with the extra info - will also need to update
 the sprite stuct to store it.
  - need to consider draw order (painters algorithm) for scene elements - kong needs to be drawn
  over ladders, ladders over girders etc.

  - update the sprite asset xml files to the new format.

  - if the computer enables vsync and my frame rates is capped then this can cause timing problems
  if my hard coded expected fps cap is different to the practival vsyc cap since my constant dt
  will not be correct - will need to handle this somehow. Otherwise the entire engine breaks
  if vsync is enabled and the game is running on a screen with a different refresh rate.

  - also implement variable fps so can run cutscenes at 24hz instead of 60hz - dont need the
  extra responsiveness for a movie so can save some CPU work.

  - need to debug the engine timing - the time on the game clock is not in sync with the actual
  game as the game is updated with the constant dt and adding up the update dt's does not equal
  the game clock time. This is evident from the inaccurate timing of the cutscenes w.r.t the 
  game clock.

  - DONE: engine timing is actually all correct I was reading the engine stats output wrong - am 
  outputting in minutes - was reading 0.1 minute to be 10s but it of course is not, its 6s, thus
  all is good but I should output the uptime and game now in a more inuitive format - I think
  [ minutes : seconds : millseconds ]
