- need to take into account different color modes when drawing sprites - at min always take
the color from the sprite resource independent of the color mode.

how to handle the font loading?

  - bitmap font generator can generate the image with the RGB channels all white (255) for the entire
    image and the alpha channel being 255 for glyph pixels and 0 for non-glyph pixels (empty space).
    I can then load this format into a sprite instance (seperating each glyph into their own sprite
    or I can store the sheet in the font and have the draw text function just use the sheet; the
    latter way will prob be better as it will mean fewer vector instances and therefore fewer
    small allocations, less fragmentation and less allocation time)
  - since I use an alpha value of 0 as the key for non-drawn pixels those empty space pixels
    when copied to a screen will not be drawn event though the RGB channels will be set to 255 
    on the screen (or I could just not copy the pixels to the screen but this would require checks
    on every pixel)
  - in color mode rgb the text would be drawn white since that is the color of each pixel in the
    bitmap, in color mode bitmaps, all pixels with alpha channel values != alpha key (i.e. 0)
    will adopt the bitmap color. This will allow me to draw fonts in any color.
  - this method will also allow me to use the banded color modes when drawing text. (side note
    that in si the ui doesnt use color banding, thus the ui layer will want to be in rgb mode 
    while the stage layer will want to be in banding mode).
  - can use tiny xml to load font data and data for each glyph then use bmpimage to load the
    glyph image (will need to convert glyph image to a bmp 32bit RGBA with gimp as bitmap font
    generator prog only exports png). The xml file contains the offset, sizes and advances etc
    I need to space all glyphs.
  - Bitmap font generator is doing a really bad job of generating a png for a small 8px font, 
    thus I think I will need to make the image myself and set the data in the xml file myself
    for the font. This will be the base little font used in the game. May be able to use 
    the font generator for larger fonts that it wont butcher, thus should copy the xml format
    so I can load both custom hand written xml font files and the ones generated by the font
    generator program.


    NEXT STEP THEN:
      HAND MAKE MY FONT IMAGE AND THE XML
      IMPLEMENT THE FONT LOADER AND FONT RENDERER IN GFX
      UPDATE THE BITMAP CLASS TO STORE THE WIDTH HEIGHT DATA IN A SIZE VECTOR2I TO BRING INLINE 
      WITH CONVENTIONS USED ELSEWHERE IN THE ENGINE CODE BASE.

  - had another idea to support animations - should change ResourceKey_t for sprites to being
    for sprite sheets and then in the function to drawSprite should include a frame parameter
    like:
        drawSprite(Vector2i position, TilesetID setid, int frame, Layer layer)
                                          ^
                                          |
                                          tileset is a better name than sprite sheet.

                                              or how about spriteset

                                              SpritesetID    <-- yh I like this even better
                                                                 since they are not really tiles
                                                                 but frames in a set, the set
                                                                 can be any set though.

    and create an xml file for the sheet which like for fonts contains meta data for each frame
    in the sheet as well as for the sheet itself, so the filename for the image, the number of
    frames in the sheet, and for the frames, the positions in the image and the frame number.

    This will make loading sprites analagous to loading fonts in that both assets use an xml file
    for meta data and an associated bmp file for the pixel data.

    This would make it easier to implement large animation sets such as needed if I were to
    implement a game like galaga which uses frames for different rotation states.

    would prob still hardcode the expected number of frames in each sheet and what each frame
    corresponds to so would have to match up the data in the xml file and the bmp file with the
    hardcoded expectations in the game code.

  - can also get rid of the sprite class and just have a tileset class which contains the pixel
    data stored in a bmpimage instance (bmpimage and sprite are basically exactly the same in 
    terms of member data), and then store the metadata for the frames within the tileset class.

  
DONE: 26th jan
  - implemented sprite sheets and their loader

TODO
  - refactor draw sprite to use the sprite sheets instead (will need to pass a frame)
  - compile, debug and test new bmpimage and gfx code
  - implement font loading and rendering
  - compile and test

TODO
  - thoroughly test the asset loaders by trying to break them - will want to verify font data
    by checking all glyph coordinates are within the bmp file to avoid later seg faults. Keep
    varying the errors in the xml file to see how my loader handles them. Want to handle them
    gracefully and avoid crashes.

  - handle the different color modes when redering and implement further rendering functions.

TODO - 29th jan
  - IT RUNS LIKE SHIT!! NEED TO IMPROVE THE PERFORMANCE OF THE RENDERING. THIS WILL NOT DO!
  - After timing some of the code it is the glClear call which is taking around 15-20ms to return.
    This thread explains why this happens:
      https://stackoverflow.com/questions/29551516/glclear-takes-too-long-android-opengl-es-2
    Basically my draw calls are too expensive. The CPU is able to issue the calls far faster than
    the GPU is able to render them, thus the glClear call is running some sync method to stall
    the CPU until the GPU can catch up. Solution is clearly to reduce the demand on the GPU.

    Good news is that my text rendering function was not the problem thus that doesn't need 
    optimising.

    note: this likely also explains why the profiling tools I was using (gprof and valgrind) were 
    not very helpful and did not reveal this problem. They were telling me it was the text rendering 
    function that was taking all the time. This is likely that it was taking all the CPU time (the 
    most expensive function run on the CPU) and the profilers were not measuring the time the CPU 
    was waiting for the GPU.

  - Ok so I reduce the demand on the GPU and it made no difference! So my next best guess would
    be that the GPU is intentionally limiting draw rate to sync with the screen. 
    pen -> yep it appears to be due to vsync being enabled on my intel integrated graphics GPU.

    If I run the engine on my old laptop with intel i3 M370 integrated graphics it runs at a 
    limit of 60Hz. Running on my current computer with intel i5 ? with HD Graphics 4600 it runs
    at a limit of 50Hz. Although note that it is the screen refresh rate which the FPS is being
    clamped to.

    This explains why timing each gl call shows the net result of the timings is always ~20ms
    (to result in 50Hz) or ~16ms (to result in 60Hz) on my old laptop. If I reduce the demand on
    the GPU the glClear call stalls, if I increase the demand on GPU via other gl calls the
    glClear call returns much faster. If I overload the other calls the glClear call returns
    in ~10us thus it is not the glClear call but the driver artifically limitiing frame rate.

    Running the 'glxgears' program from the 'mesa-demos' (arch) package shows this is the case.
    The demo renders some gears at the capped vsync rate. The rate capped to in the demo matches
    the rate my engine is capped to on both this machine and my old laptop.

    This post explains how to lift the vsync restriction:
      https://www.microbasic.net/2015/04/how-to-disable-vsync-on-intel-card-linux/

    It is interesting to note however that the vsync seems to be doing a rather poor job as the
    rendering is very flickery on this machine at 50Hz whereas on my old laptop at 60Hz it is
    not. Thus I can only conlude this machines vsync is actually out of sync.

    Yup running the engine example with:
      $ vblank_mode=0 ./example

    runs it without vsync and it runs at capped tick rate with FPS of 980Hz. Without the 
    vsyn the current example runs at a cap of ~750Hz. So performance is not that bad after all.

    +- I was wrong on the vsync being out of sync. Movement is still very flickery even with 
    |  unlimited frame rate. However on my old laptop it is not. The difference is that on my old
    |  laptop the moving object clearly jumps 1 virtual pixel at a time (which on the scale of the
WRONG  demo is ~6 real pixels) thus rather than smoothly moving across the screen it looks like it
    |  is hopping across at regular intervals. On this machine (the newer one) the movement is smooth
    |  with no hopping. You cannot tell that it is actually moving 6px at a time, but it looks very
    |  flickery as a result. My guess is that there is some sort of GPU feature that is trying to
    +- smooth the movement. Either that or it is the screen.

          the flickeryness is actually just a product of the movement speed it would appear. The
          object on my laptop was running slower because in my engine the update rate was 1000Hz
          but the vsync was clamping it to 60Hz, thus if I integrate over a dt of 1/1000 secs but
          do so only 60 time a second ... things move slower and thus flicker less -> dipshit ey!

     +-    BOOM! just discovered that if I lock the engine to draw at the same rate of the screen then
     |     the Graphics driver does not enable vsync, if I try to draw at a higher rate than the screen
     |     refresh rate it does. Thus if I lock my engine to only draw the game at 60Hz and I am running
 WRONG     the game on a screen that draws at 60Hz then simply running ./example without the vblank_mode=0
     |     still results in no vsync. If I then lock the engine draws to 70Hz and run ./example then
     |     the GPU will start blocking in the gl calls again to lock the draw rate the the 60Hz monitor
     |     refresh rate. This is why I was encountering this problem. Because I was drawing at 60Hz but 
     +-    the monitor I was running on refeshed at 50Hz thus the GPU driver was enabling vsync. -> NOPE!
           wrong it does start blocking still. So ignore that theory :). 

            I will leave it here to remind me its not the case.

 Conclusion of all this, just disable vsync during development and carry on.


 - impelement the animation modes
 - setup drawing sprites from the center position rather than the bottom-left - pass the center into
  the gfx draw function? Make a seperate function? gfx::drawSpriteCentered(Vector2i position, ...)
 - update the loader to load the new sprite xml format with the extra info - will also need to update
 the sprite stuct to store it.
  - need to consider draw order (painters algorithm) for scene elements - kong needs to be drawn
  over ladders, ladders over girders etc.

  - update the sprite asset xml files to the new format.

  - if the computer enables vsync and my frame rates is capped then this can cause timing problems
  if my hard coded expected fps cap is different to the practival vsyc cap since my constant dt
  will not be correct - will need to handle this somehow. Otherwise the entire engine breaks
  if vsync is enabled and the game is running on a screen with a different refresh rate.

  - also implement variable fps so can run cutscenes at 24hz instead of 60hz - dont need the
  extra responsiveness for a movie so can save some CPU work.

  - need to debug the engine timing - the time on the game clock is not in sync with the actual
  game as the game is updated with the constant dt and adding up the update dt's does not equal
  the game clock time. This is evident from the inaccurate timing of the cutscenes w.r.t the 
  game clock.

  - DONE: engine timing is actually all correct I was reading the engine stats output wrong - am 
  outputting in minutes - was reading 0.1 minute to be 10s but it of course is not, its 6s, thus
  all is good but I should output the uptime and game now in a more inuitive format - I think
  [ minutes : seconds : millseconds ]

The next week or so:

  - implement a simple particle system - want to add particle effects to the engine splash screen
    spawning particles along the line of pixel columns appearing so its looks like a wave of a
    magic wand summoning the letters.

  - create loaders for .wav and .ogg sound files. refs:
  http://soundfile.sapp.org/doc/WaveFormat/

  - implement a sound system/module in the sytle of the gfx module using openal. Some good refs:
    https://github.com/kcat/openal-soft/wiki/Programmer%27s-Guidehttps://github.com/kcat/openal-soft/wiki/Programmer%27s-Guide
    https://indiegamedev.net/2020/02/15/the-complete-guide-to-openal-with-c-part-1-playing-a-sound/
    https://indiegamedev.net/2020/02/25/the-complete-guide-to-openal-with-c-part-2-streaming-audio/
    https://indiegamedev.net/2020/01/16/how-to-stream-ogg-files-with-openal-in-c/
    https://indiegamedev.net/2020/01/23/game-engine-development-for-the-hobby-developer-part-3-audio/
    https://github.com/kcat/openal-soft

    want an audio streaming system so can stream music files whilst I play them. May not be
    needed however since the music in these classic arcade games is prob short files played on
    loop so can likely just load them into RAM in full. But want to do anyway just for fun.

 - write blog posts on the streamer and the audio file loaders? also setup the blog post for 
 the bmp loader. I have already written the article pretty much.

- idea: learn about concurrency and make the engine multi-threaded - can have a driving main
game logic thread which runs the logic and issue draw and audio requests (the draw requests
can be queued and software rendered to the virtual screen via the render thread thus the render
thread just copies pixels all the time) and the audio thread can handle streaming music and 
sounds etc. To do this would likely want to make the virtual screens double buffered I think so
can draw the last frame whilst rendering the next. This would allow lots more drawing I think
whilst still maintaining a high fps. Can I record the performance of each thread. 

This is actually not a new idea (as one would expect), it is pretty common to parallize the renderer
since as a system its output does not feed into other systems. See this thread:
https://www.reddit.com/r/gamedev/comments/96v3f4/concurrency_and_game_engines/
This answer I found really useful as it talks about exactly what I was already thinking, especially
with the adding of latency.

"
Hi, concurrency is a thorny problem in games mostly because there is a large and varied quantity of feedback from one update to the next. This creates a model of computation where the game is like a database occasionally acted upon by "queries", with a large amount of synchronization logic used to ensure that behaviors remain stable. Every time you're working with an FSM or employing a buffer to handle events later in your main loop or spawning and despawning objects, that's a potential synchronization point. Having to manage synchronization is an indicator that we are working with behaviors that are fundamentally concurrent, but a game engine does so in an informal fashion, and most problems are solved by cascading one form of feedback into the next in a predetermined order, so that each step of processing has exactly what it needs, and not some kind of mishmash of partially read or written data.
Creating a high-concurrency engine out of this model is a matter of clarifying the synchronization, step by step, so that it can be formalized into asynchronous jobs that run independently. This creates a seemingly unending number of engineering puzzles - and it's definitely not the way you'd want to approach your prototyping, since it's way more succinct to address it as a fixed sequence. Shipping engines that take on the challenge are generally optimizing hotspots, but not the whole thing, so their ability to work concurrently is limited. Rendering happens to be exceptional to this in that rendering does not feed into other subsystems(generally speaking), so it can work on data in any order needed. Hence a lot of games will tack on a parallelized renderer.
This brings to mind the most common trick used to get more parallel utilization: add latency. If you build in a predefined notion of latency, you can work on multiple frames at the same time, always kicking off a new one while presenting an old one. So a number of current engines always show gameplay that's a frame behind the main update loop. Gameplay logic also tends to add frames of latency to some events for reasons of convenience. It's never totally ideal to do this since it impacts responsiveness, but it is most likely "the future" if we take audio as the example and extrapolate(buffers of hundreds to thousands of samples rendered ahead of the one being presented).
"

This would be a good idea since I would want to demonstrate knowledge of multithreading when I
apply to jobs and I dont want to do another project to do that.

- So learn about concurrency over the next week along with audio and the other stuff listed above.
https://www.youtube.com/watch?v=wXBcwHwIt_I - chero vid
https://www.youtube.com/watch?v=5HWCsmE9DrE

- an experimentation and learning week me thinks.

https://www.youtube.com/watch?v=p9pnzz0alk0 
-  the above vid around the 7min mark talks about the limitations on the audio thread and what
requirements it must meet, such as never allocating memory and never locking the thread. Should
learn more about this.

- USE THE C++17 FALLTHROUGH STATEMENT IN MY ENGINE EVENT LOOP IN STEAD OF THE COMMENT - JUST
BECAUSE ITS A NEW FEATURE I NEVER NEW EXISTED (1ST SAW THE IN CPPCON AUDIO VID), REF
https://en.cppreference.com/w/cpp/language/attributes/fallthrough 
note: will need to compile with -std=c++17 which I think I already am.

- TODO - also setup a build system to build tinyxml2 as a lib and link, and my engine as a lib
and link and prob also openal as a lib and link - will want to link to the game elf object. Will
use make (and maybe bash) to setup this build system.

- the errors in openAL are a mess to handle - and to make matters worse (or better) many of them
will never happen if you setup the library correctly - thus should I handle all errors or not?
If I strip some of the error handling out then the code will be much simpler and prob a little
faster. Should go through the functions and look at what errors should be handled and what 
shouldn't.

- read the openAL source + spec (some of it) and improve my sound engine if needed - compile and
test and debug -- DONE

- add the ability to start and stop sounds - and ability to have sound loop - to stop sounds may
need to track which sounds (resource keys) are playing in which sources so can stop the source
playing. - note: when implementing donkey kong will not need to stream music into a queued source
as the music tracks are very short repetitive sounds that can just be looped. - DONE

- add sound support to the cutscene system so can play sounds at particular times.

- setup the build system - organising libraries into seperate dirs etc.

- also implement the performance graphs for the engine stats

- strip the color modes from the gfx module - these would be better implemented as shader functions
 in which I call the shader function when assigning the color - pass the input color and pixel
 position into a shader function - assign the output color of the function to the vscreen.
